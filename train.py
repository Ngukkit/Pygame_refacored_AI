from agent import DQNAgent
from game_env import MyGameEnv
import torch
import numpy as np
import json
import os
import re

# ê²Œì„ í™˜ê²½ê³¼ ì—ì´ì „íŠ¸ ì—°ê²°
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'
env = MyGameEnv(render_mode=True)
state_size = len(env.observe_state())  # ìƒíƒœ ë²¡í„° í¬ê¸°
state = env.observe_state()
action_size = 6  # ì˜ˆì‹œ: 5ê°€ì§€ í–‰ë™ (ì´ë™, ê³µê²© ë“±)
goal_indices = []
# ë°œíŒ ìœ„ ì—¬ë¶€ (4ê°œ)
goal_indices += [10 + 5*i + 3 for i in range(4)]
# ì¹´ìš´íŠ¸ë‹¤ìš´
goal_indices.append(30)
# ëª¬ìŠ¤í„° ì²´ë ¥ (8ë§ˆë¦¬)
goal_indices += [31 + 8*i + 7 for i in range(5)]
# ì•„ì´í…œ ì¡´ì¬ ì—¬ë¶€ (5ê°œ)
goal_indices += [71 + 4*i + 1 for i in range(5)]
# ì•„ì´í…œ íšë“ ì—¬ë¶€ (5ê°œ)
goal_indices += [71 + 4*i + 2 for i in range(5)]
# í¬íƒˆ ìƒì„± ë° ì¶©ëŒ ìƒíƒœ (2ê°œ)
goal_indices += [91, 92]

action_descriptions = {
    0: "<---",   # ì˜ˆ: '0'ì´ë©´ 'ê³µê²©'ì„ ìˆ˜í–‰
    1: "--->",   # ì˜ˆ: '1'ì´ë©´ 'ì í”„'ë¥¼ ìˆ˜í–‰
    2: "Jump",  # ì˜ˆ: '2'ì´ë©´ 'ìŠ¤í‚¬1'ì„ ìˆ˜í–‰
    3: "attack",  # ì˜ˆ: '3'ì´ë©´ 'ìŠ¤í‚¬2'ë¥¼ ìˆ˜í–‰
    4: "idle",  # ì˜ˆ: '4'ì´ë©´ 'ìŠ¤í‚¬3'ì„ ìˆ˜í–‰
    5: "use skill1",  # ì˜ˆ: '5'ì´ë©´ 'ìŠ¤í‚¬4'ë¥¼ ìˆ˜í–‰
}

agent = DQNAgent(state_size, action_size,gamma=0.99, lr=0.0003, goal_indices=goal_indices)
model_dir = "."

model_files = [f for f in os.listdir(model_dir) if re.match(r"dqn_model(\d+).pth", f)]

if model_files:
    # ëª¨ë¸ íŒŒì¼ ì´ë¦„ì—ì„œ ìˆ«ì ë¶€ë¶„ì„ ì¶”ì¶œí•˜ì—¬ ì •ìˆ˜ë¡œ ë³€í™˜ í›„, ê°€ì¥ í° ìˆ«ìë¥¼ ì°¾ìŒ
    epoch_numbers = [int(re.search(r"(\d+)", f).group(1)) for f in model_files]
    latest_epoch = max(epoch_numbers)
    
    # ê°€ì¥ ë†’ì€ ìˆ«ìì˜ ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
    latest_model_path = f"dqn_model{latest_epoch}.pth"

    # ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
    agent.load(latest_model_path)
    print(f"ëª¨ë¸ {latest_epoch}ì„(ë¥¼) ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
else:
    print("ëª¨ë¸ íŒŒì¼ì´ ì—†ì–´ ìƒˆë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
   
# ì—í”¼ì†Œë“œ ìˆ˜
episodes = 3000
start_epoch = latest_epoch if model_files else 0

# device = torch.device("cpu")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("CUDA ì‚¬ìš© ê°€ëŠ¥?", torch.cuda.is_available())
if torch.cuda.is_available():
    print("ì‚¬ìš© ì¤‘ì¸ GPU:", torch.cuda.get_device_name(0))

# ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
agent.model.to(device)

# ìƒíƒœë¥¼ Tensorë¡œ ë³€í™˜í•˜ê³  ë””ë°”ì´ìŠ¤ì— ë§ì¶° ì´ë™
state_tensor = torch.tensor(state).unsqueeze(0).float().to(device)

for e in range(start_epoch,start_epoch + episodes):

    state = env.reset()
    done = False
    total_reward = 0
    step_count = 0
    reward = 0 
    last_reward_step = 0  # ë§ˆì§€ë§‰ ë³´ìƒ ìŠ¤í… ì´ˆê¸°í™”
        
    while not done:
                
        action = agent.act(state,reward = reward,use_reward_simulation=False)
        action_description = action_descriptions.get(action, "ì•Œ ìˆ˜ ì—†ëŠ” ì•¡ì…˜")

    # ë‹¤ìŒ ì•¡ì…˜ì„ ìœ„í•´ ë§ˆì§€ë§‰ ì•¡ì…˜ ì‹œê°„ ê°±ì‹ 
        next_state, reward, done, _ = env.step(action)
        
        # latest_epoch = max(epoch_numbers) if model_files else 0

        step_count +=1

        if (step_count - last_reward_step) % 500 == 0 and step_count != last_reward_step:
            print(f"total steps: {step_count - last_reward_step} ìŠ¤í… epsilon UP ")
            # agent.epsilon = min(agent.epsilon + 0.005, 1)
            # print(f"postion : x{env.px} ,y{env.py}\n")
            reward = -0.2
       
        if step_count - last_reward_step == 1500:
            print(f"ë³´ìƒ ì—†ì´ 1,500 ìŠ¤í… ì´ˆê³¼, x{env.px} ,y{env.py}")
            reward = -0.5
  
        if step_count - last_reward_step >= 3000:
            print(f"ë³´ìƒ ì—†ì´ 3,000 ìŠ¤í… ì´ˆê³¼, ì—í”¼ì†Œë“œ ê°•ì œ ì¢…ë£Œ x{env.px} ,y{env.py}")
            reward = -1.0
            done = True
        
        # if reward >= 0.05:
        #     last_reward_step = step_count
            # agent.epsilon = max(agent.epsilon_min, agent.epsilon * 0.999) # ë³´ìƒ ë°›ìœ¼ë©´ íƒí—˜ ê°ì†Œ
        if reward > 0:
            # print(f" reward: {reward}  x{env.px} ,y{env.py}")
            last_reward_step = step_count
            
        total_reward += reward
        
        env.last_reward_timer(step_count-last_reward_step,MAXCOUNTDOWN=3000)  # ë³´ìƒ íƒ€ì´ë¨¸ ì„¤ì •

        # ğŸ”¹ ê²½í—˜ ì €ì¥     
        agent.remember(state, action, reward, next_state, done) 
        
        # ê²½í—˜ ë¦¬í”Œë ˆì´
        if len(agent.memory) >= 50 and step_count % 100 == 0:
            agent.replay(batch_size=50)  
        
        # ğŸ”¹ íƒ€ê²Ÿ ëª¨ë¸ ì—…ë°ì´íŠ¸
        if step_count % 300 == 0:
            agent.update_target_model() 
        
        # ğŸ”¹ ìŠ¤ì¼€ì¤„ ê°±ì‹ 
        agent.update_schedules()
        
        if total_reward < 0:
            print("reward is Zero, episode end")
            done = True
            
        if step_count % 50 == 0:
            print(f"epsilon:{agent.epsilon:.4f},q_values:{agent.q_valuefordp:.4f},x{env.px:3.0f},y{env.py:3.0f},lamda:{agent.lambda_decay:.2f},Reward:{total_reward:.3f},Action:{action_description}")

        # ì£½ì—ˆì„ë•Œ ëª¨ë¸ ì €ì¥
        if done:
            # agent.epsilon = min(agent.epsilon_max, agent.epsilon * 1.01)  # ë˜ëŠ” 1.05 ì¦ê°€ë„ ê°€ëŠ¥
            # agent.epsilon = min(agent.epsilon_max, 0.5) # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì‹œ íƒí—˜ ì¦ê°€
        
            q_log = {
                "step": step_count,
                "q_values": agent.model(state_tensor).detach().cpu().tolist()
                }  # Q ê°’ ë¡œê·¸      
            with open("q_values_log.json", "a") as f:
                json.dump(q_log, f)
                f.write("\n")
                
        state = next_state        

    if (e + 1) % 10 == 0 and agent.epsilon > 0.9:
        save_path = rf"dqn_model{e+1}.pth"
        agent.save(save_path)
        print(f"{save_path} ì €ì¥ ì™„ë£Œ.")
    elif (e + 1) % 50 == 0 and 0.9 >= agent.epsilon > 0.7:
        save_path = rf"dqn_model{e+1}.pth"
        agent.save(save_path)
        print(f"{save_path} ì €ì¥ ì™„ë£Œ.")
    elif (e + 1) % 100 == 0 and 0.7 >= agent.epsilon:
        save_path = rf"dqn_model{e+1}.pth"
        agent.save(save_path)
        print(f"{save_path} ì €ì¥ ì™„ë£Œ.")

    print(f"Episode {e+1}, End Total Reward: {total_reward:.3f}")

agent.save(r"dqn_model.pth")
